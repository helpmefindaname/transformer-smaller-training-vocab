

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>transformer_smaller_training_vocab &#8212; transformer-smaller-training-vocab 0.3.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/github_style.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'api/transformer_smaller_training_vocab';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Api docs" href="../modules.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    <p class="title logo__title">transformer-smaller-training-vocab 0.3.1 documentation</p>
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../getting-started.html">
                        Getting started
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../modules.html">
                        Api docs
                      </a>
                    </li>
                
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../getting-started.html">
                        Getting started
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../modules.html">
                        Api docs
                      </a>
                    </li>
                
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  <div>
    <h3><a href="../index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">transformer_smaller_training_vocab</a><ul>
<li><a class="reference internal" href="#transformer_smaller_training_vocab.get_texts_from_dataset"><code class="docutils literal notranslate"><span class="pre">get_texts_from_dataset()</span></code></a></li>
<li><a class="reference internal" href="#transformer_smaller_training_vocab.recreate_vocab"><code class="docutils literal notranslate"><span class="pre">recreate_vocab()</span></code></a></li>
<li><a class="reference internal" href="#transformer_smaller_training_vocab.reduce_train_vocab"><code class="docutils literal notranslate"><span class="pre">reduce_train_vocab()</span></code></a></li>
<li><a class="reference internal" href="#transformer_smaller_training_vocab.reduce_train_vocab_and_context"><code class="docutils literal notranslate"><span class="pre">reduce_train_vocab_and_context()</span></code></a></li>
</ul>
</li>
</ul>

  </div></div>
        <div class="sidebar-primary-item">
<div id="searchbox"></div></div>
        <div class="sidebar-primary-item">
<h3>Versions</h3>
<ul>
  <li><a href="transformer_smaller_training_vocab.html">
    
    latest (dev)
    
    
    [x]
    
  </a></li>
  <li><a href="../../0.3.1/api/transformer_smaller_training_vocab.html">
    
    0.3.1
      
      (stable)
      
    
    
  </a></li>
  <li><a href="../../0.3.0/api/transformer_smaller_training_vocab.html">
    
    0.3.0
      
    
    
  </a></li>
  <li><a href="../../0.2.4/api/transformer_smaller_training_vocab.html">
    
    0.2.4
      
    
    
  </a></li>
</ul>
</div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumbs">
  <ul class="bd-breadcrumbs" role="navigation" aria-label="Breadcrumb">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../modules.html" class="nav-link">Api docs</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">transformer_smaller_training_vocab</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  

<p>
  <strong>
    
    You're reading the documentation for a development version.
    For the latest stable version, please have a look at <a href="../../0.3.1/api/transformer_smaller_training_vocab.html">0.3.1</a>.
    
  </strong>
</p>


  <section id="module-transformer_smaller_training_vocab">
<span id="transformer-smaller-training-vocab"></span><h1>transformer_smaller_training_vocab<a class="headerlink" href="#module-transformer_smaller_training_vocab" title="Permalink to this heading">#</a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="transformer_smaller_training_vocab.get_texts_from_dataset">
<span class="sig-prename descclassname"><span class="pre">transformer_smaller_training_vocab.</span></span><span class="sig-name descname"><span class="pre">get_texts_from_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/helpmefindaname/transformer-smaller-training-vocab/blob/main/transformer_smaller_training_vocab/utils.py#L13-L40"><span class="linkcode-link"><span class="pre">View</span> <span class="pre">on</span> <span class="pre">GitHub</span></span></a><a class="headerlink" href="#transformer_smaller_training_vocab.get_texts_from_dataset" title="Permalink to this definition">#</a></dt>
<dd><p>Extract the texts of a dataset given their keys.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is only available if the <cite>datasets</cite>-extra is installed.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DatasetDict</span></code>]) – The huggingface dataset used for training.</p></li>
<li><p><strong>key</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]) – Either a simple string, being the key referring the text or a Tuple of two strings,
referring to the keys for a text pair</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]]]</p>
</dd>
</dl>
<p>Returns: the texts or text pairs extracted from the dataset</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_smaller_training_vocab.recreate_vocab">
<span class="sig-prename descclassname"><span class="pre">transformer_smaller_training_vocab.</span></span><span class="sig-name descname"><span class="pre">recreate_vocab</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">used_tokens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">saved_vocab</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">saved_embeddings</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">empty_cuda_cache</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/helpmefindaname/transformer-smaller-training-vocab/blob/main/transformer_smaller_training_vocab/contextual_reduce.py#L75-L105"><span class="linkcode-link"><span class="pre">View</span> <span class="pre">on</span> <span class="pre">GitHub</span></span></a><a class="headerlink" href="#transformer_smaller_training_vocab.recreate_vocab" title="Permalink to this definition">#</a></dt>
<dd><p>Recreates the full vocabulary from a reduced model.</p>
<p>Combines the stored embeddings with the updated embeddings of the reduced model and stores everything in place
to have a model functioning on full vocabulary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code>) – The reduced transformers model to recreate</p></li>
<li><p><strong>tokenizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code>) – The reduced tokenizer to recreate</p></li>
<li><p><strong>used_tokens</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The ids of tokens that are still contained</p></li>
<li><p><strong>saved_vocab</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The full vocabulary that was saved.</p></li>
<li><p><strong>saved_embeddings</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The saved embeddings of the full transformer before training.</p></li>
<li><p><strong>empty_cuda_cache</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – Defaults to True if the model is stored on cuda and False otherwise.
If False, for some time, the weights will be in memory twice (Full + Reduced),
before the garbage collection removes the Full weights from cache.
If True, the cache will be emptied, before the reduced weights will be loaded to the device of the model and
therefore won’t have a temporarily higher memory footprint.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_smaller_training_vocab.reduce_train_vocab">
<span class="sig-prename descclassname"><span class="pre">transformer_smaller_training_vocab.</span></span><span class="sig-name descname"><span class="pre">reduce_train_vocab</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">texts</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">empty_cuda_cache</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/helpmefindaname/transformer-smaller-training-vocab/blob/main/y#L108-L144"><span class="linkcode-link"><span class="pre">View</span> <span class="pre">on</span> <span class="pre">GitHub</span></span></a><a class="headerlink" href="#transformer_smaller_training_vocab.reduce_train_vocab" title="Permalink to this definition">#</a></dt>
<dd><p>Contextmanager to temporary reduce the model for training.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">reduce_train_vocab</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">texts</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># train reduced model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># save full model again</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code>) – The transformers model to reduce</p></li>
<li><p><strong>tokenizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code>) – The tokenizer respective to the transformers model</p></li>
<li><p><strong>texts</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]]]) – A Sequence of either texts, pre-tokenized texts, text-pairs or pre-tokenized textpairs.
Usually the full training + validation data used when training. The model &amp; tokenizer vocabulary will be reduced
to only tokens that are found in those texts.</p></li>
<li><p><strong>empty_cuda_cache</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – Defaults to True if the model is stored on cuda and False otherwise.
If False, for some time, the weights will be in memory twice (Full + Reduced),
before the garbage collection removes the Full weights from cache.
If True, the cache will be emptied, before the reduced weights will be loaded to the device of the model and
therefore won’t have a temporarily higher memory footprint.</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>]) – Defaults to None
If provided, the optimizer parameters will be updated, to use the reduced embeddings instead of the old pointer.
It is crucial to provide the optimizer if one was created before reducing the model.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_smaller_training_vocab.reduce_train_vocab_and_context">
<span class="sig-prename descclassname"><span class="pre">transformer_smaller_training_vocab.</span></span><span class="sig-name descname"><span class="pre">reduce_train_vocab_and_context</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">texts</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">empty_cuda_cache</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/helpmefindaname/transformer-smaller-training-vocab/blob/main/transformer_smaller_training_vocab/contextual_reduce.py#L15-L72"><span class="linkcode-link"><span class="pre">View</span> <span class="pre">on</span> <span class="pre">GitHub</span></span></a><a class="headerlink" href="#transformer_smaller_training_vocab.reduce_train_vocab_and_context" title="Permalink to this definition">#</a></dt>
<dd><p>Reduce the vocabulary given a set of texts.</p>
<p>Reduces the vocabulary of a model and a tokenizer by checking which tokens are used in the text
and discarding all unused tokens.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code>) – The transformers model to reduce</p></li>
<li><p><strong>tokenizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code>) – The tokenizer respective to the transformers model</p></li>
<li><p><strong>texts</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]]]) – A Sequence of either texts, pre-tokenized texts, text-pairs or pre-tokenized textpairs.
Usually the full training + validation data used when training. The model &amp; tokenizer vocabulary will be reduced
to only tokens that are found in those texts.</p></li>
<li><p><strong>empty_cuda_cache</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – Defaults to True if the model is stored on cuda and False otherwise.
If False, for some time, the weights will be in memory twice (Full + Reduced),
before the garbage collection removes the Full weights from cache.
If True, the cache will be emptied, before the reduced weights will be loaded to the device of the model and
therefore won’t have a temporarily higher memory footprint.</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>]) – Defaults to None
If provided, the optimizer parameters will be updated, to use the reduced embeddings instead of the old pointer.
It is crucial to provide the optimizer if one was created before reducing the model.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>All information required to restore the original vocabulary after training, consisting of:</p>
<ul class="simple">
<li><p><strong>used_tokens</strong> (List[int]) - The ids of all tokens that will be kept in vocabulary.</p></li>
<li><p><strong>saved_vocab</strong> (Dict[str,int]) - The original vocabulary to recreate the tokenizer.</p></li>
<li><p><strong>saved_embeddings</strong> (Tensor) - The original embedding weights.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</section>



                </article>
              
              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../modules.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Api docs</p>
      </div>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer_smaller_training_vocab.get_texts_from_dataset"><code class="docutils literal notranslate"><span class="pre">get_texts_from_dataset()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer_smaller_training_vocab.recreate_vocab"><code class="docutils literal notranslate"><span class="pre">recreate_vocab()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer_smaller_training_vocab.reduce_train_vocab"><code class="docutils literal notranslate"><span class="pre">reduce_train_vocab()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer_smaller_training_vocab.reduce_train_vocab_and_context"><code class="docutils literal notranslate"><span class="pre">reduce_train_vocab_and_context()</span></code></a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div class="tocsection sourcelink">
    <a href="../_sources/api/transformer_smaller_training_vocab.rst.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      © Copyright 2023, Benedikt Fuchs.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.13.3.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>